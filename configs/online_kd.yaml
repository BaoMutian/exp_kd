# Online KD (On-Policy Knowledge Distillation) configuration
# True on-policy distillation where teacher uses Q+E and student uses Q

# Inherit from base configuration
defaults:
  - base

# Override model settings
model:
  student_path: "Qwen/Qwen3-8B"
  teacher_path: "Qwen/Qwen3-8B"

# Online KD specific settings
online_kd:
  # Number of responses to sample from student per input
  # Higher values = more diverse training signal, but slower
  # Recommended: 1-4
  num_samples: 1
  
  # Temperature for KL divergence computation
  temperature: 1.0
  
  # Maximum tokens to generate per response
  # WARNING: Large values cause very slow training due to autoregressive generation
  # Each token requires one full forward pass!
  # Recommended: 128-256 for reasonable training speed
  max_new_tokens: 128
  
  # Divergence type
  # "forward": KL(teacher || student) - mean-seeking, covers all modes
  # "reverse": KL(student || teacher) - mode-seeking, focuses on high-prob
  # "jsd": Generalized Jensen-Shannon Divergence
  kl_type: "jsd"
  
  # Beta for JSD interpolation (only used when kl_type="jsd")
  # beta=0.0: approximates forward KL
  # beta=1.0: approximates reverse KL
  # beta=0.5: balanced JSD
  beta: 0.5

# Generation settings for on-policy sampling (Qwen3 non-thinking mode)
generation:
  temperature: 0.7
  top_p: 0.8
  top_k: 20
  do_sample: true

# Training configuration overrides
training:
  output_dir: "outputs/online_kd"
  
  num_train_epochs: 3
  
  # Online KD requires more memory due to generation
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  
  # Lower learning rate for stability
  learning_rate: 5.0e-6
  
  # Gradient clipping
  max_grad_norm: 1.0
  
  # Evaluation settings
  eval_strategy: "no"

