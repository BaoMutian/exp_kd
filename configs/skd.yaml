# SKD (Supervised Knowledge Distillation) configuration
# SKD trains the student to match teacher's token-level probability distribution
# using KL divergence on ground-truth sequences

# Inherit from base configuration
defaults:
  - base

# Override model settings if needed
model:
  student_path: "Qwen/Qwen3-8B"
  teacher_path: "Qwen/Qwen3-8B"

# SKD specific settings
skd:
  # Temperature for softening probability distributions
  # Higher temperature -> softer distributions -> more knowledge transfer
  # Recommended: 2.0-4.0 for distillation
  temperature: 4.0
  
  # Alpha: weight for the distillation loss vs. standard CE loss
  # total_loss = alpha * kl_loss + (1 - alpha) * ce_loss
  # Lower alpha = more weight on CE loss (more stable)
  # Recommended: 0.3-0.5 for stability
  alpha: 0.5
  
  # KL divergence direction
  # "forward": KL(p_teacher || p_student) - mean-seeking, covers all modes
  # "reverse": KL(p_student || p_teacher) - mode-seeking, focuses on high-prob tokens
  # Recommended: "reverse" is often more stable for same-size teacher/student
  kl_direction: "reverse"

# Training configuration overrides
training:
  output_dir: "outputs/skd"
  
  num_train_epochs: 3
  
  # SKD requires loading both teacher and student, may need smaller batch size
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  
  # Lower learning rate for distillation (important for stability)
  learning_rate: 5.0e-6
  
  # Stricter gradient clipping for SKD
  max_grad_norm: 0.5

