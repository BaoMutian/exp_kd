# Accelerate configuration for distributed training
# Supports multi-GPU training on 8x A100 (90GB) setup

compute_environment: LOCAL_MACHINE
debug: false
distributed_type: DEEPSPEED
downcast_bf16: 'no'
enable_cpu_affinity: false

# DeepSpeed configuration
deepspeed_config:
  deepspeed_config_file: configs/deepspeed_config.json
  zero3_init_flag: true

# Machine configuration
machine_rank: 0
main_training_function: main
# mixed_precision is configured in deepspeed_config.json, not here
num_machines: 1
num_processes: 8  # 8 GPUs
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false

