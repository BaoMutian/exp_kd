# Knowledge Distillation

### 概念与分类
知识蒸馏（KD）是用大模型（或能力更强的模型）的“理解力”来引导小模型（弱模型）的训练，从而实现模型轻量化而不显著牺牲性能。以下模型均指自回归的大语言模型（LLM）。

知识蒸馏的实现主要包括SKD、SeqKD和 GKD 三种方法。

### 1. SKD (Supervised KD, 监督式蒸馏)
**核心逻辑**：不管学生想说什么，**强迫学生模仿教师在Ground-Truth上的Token级输出概率分布。**

+ 特点：利用了教师模型在正确答案上的丰富信号，但学生模型在训练时只见过完美历史上下文。
+ 优化目标：在固定的真实数据上，**最小化教师和学生Token级概率分布之间的前向 KL 散度**。
+ Loss 函数：

$ \mathcal{L}_{SD}(\theta) := \mathbb{E}_{(x, y) \sim (X, Y)} \left[ \mathcal{D}_{KL}(p_T || p_S^\theta)(y|x) \right] $

**SKD比SFT的优势**：如果我们在没有教师的情况下用标准的SFT训练（Standard Fine-Tuning），使用的是 One-Hot 作为硬标签。而SKD 利用了教师在完整词表上的概率分布，这意味着学生模型不仅学到了“正确答案是什么”，还学到了“错误答案之间谁更接近正确”。这种关于错误类别之间关系的分布信息，就是所谓的暗知识。  

### 2. SeqKD (Sequence-Level KD, 序列级蒸馏)  
**核心逻辑**：老师先自己写一遍，然后让学生模仿老师写的内容。相当于在老师生成的数据上做硬监督。

+ **原理：** 首先让教师模型针对输入生成输出序列（这些序列可能比 Ground-truth 标签质量更好或更符合模型偏好），然后将这些教师生成的 Token 序列作为“伪标签”数据，让学生模型进行有监督微调（SFT）。
+ **特点：** 它可以被视为在教师生成数据上的 SFT，通常能让学生更好地学习教师的风格，但生成大量教师序列可能计算成本很高 。
+ **优化目标**：使学生模型最大化教师模型生成的响应序列的似然概率。
+ **Loss 函数**：

$ \mathcal{L}_{SeqKD}(\theta) = \mathbb{E}_{(x, y_{T}) \sim (X, Y_{T})} \left[ -\log p_S^\theta(y_T|x) \right] $

- 注：公式中 $ y_T $ 代表由教师模型 $ p_T $ 针对输入 $ x $ 生成的高概率序列

### 3. GKD (广义知识蒸馏)
GKD为解决传统KD方法存在的问题而提出：

+ 分布不匹配：SKD 和 SeqKD 都在固定的高质量序列上训练（如人工标注数据或教师生成的序列）。然而，学生模型在推理时是自回归生成的，这导致推理时遇到的序列分布与训练时看到的固定分布可能不同（Exposure Bias）。学生模型在训练时，输入的上文总是正确的，但在推理时，学生模型必须基于自己之前生成的（可能包含错误的）上文来预测下一个词。一旦学生犯错，因为从未学过如何从错误中恢复，会造成错误积累。 
+ 目标函数局限：传统 KD 通常最小化前向 KL 散度。当前向 KL 强迫能力较弱的学生模型去覆盖教师模型的所有分布时，可能会导致学生模型生成低概率的、低质量的样本（幻觉），因为它无法完美模仿教师的复杂分布。

GKD 是一个统一的框架，它允许灵活选择 **训练数据来源** 和 **损失函数（差异度量）**。

#### **A. 在线学习（On-Policy Learning）**
GKD 不仅仅依赖固定数据集，而是从**学生模型当前策略** $ p_S $ 中采样输出序列 $ y $。

+ **原理：** 学生模型针对输入 $ x $生成响应$ y $。教师模型**对这些学生生成的序列**进行评估（提供 token 级的概率分布作为打分的“软标签”）。
+ **优化目标**：让学生模型在自己生成的输出序列上，利用教师模型的反馈进行训练，学生模型根据教师的反馈调整参数，学习如何从自己生成的（可能包含错误的）序列中恢复，从而减少训练与推理之间的分布不匹配。
+ **Loss 函数**：

$ \mathcal{L}_{GKD}(\theta) := \mathbb{E}_{x\sim X}[\mathbb{E}_{y\sim p_S(\cdot|x)}[\mathcal{D}(p_T || p_S^\theta)(y|x)]] $

+ **优势：** GKD 类似于强化学习，它是一个反馈循环。随着学生模型能力的提升，它生成的数据分布也会发生变化，GKD 会持续在新的分布上进行蒸馏。

在计算梯度时，不通过学生模型的采样分布 $ p_S(\cdot|x) $ 进行反向传播，仅对 $ p_S^\theta $ 计算梯度。这意味着：**在计算梯度更新模型参数时，我们将学生模型自己采样生成的序列看作是“固定的”训练数据，而不去追究“这个序列是怎么被选出来的”。**

#### **B. 灵活的差异度量（Divergence）**
GKD 允许使用不同的散度函数来衡量学生与教师分布的差异，而不仅仅是前向 KL：

+ **前向 KL (Forward KL)：** 传统的最大似然估计。
+ **反向 KL (Reverse KL)：** 具有“寻模”（mode-seeking）特性，促使学生模型专注于教师模型的高概率区域，有助于减少幻觉，但可能会降低多样性。
+ **广义 JSD (Generalized JSD)：** 介于前向和反向 KL 之间，是一个有界的度量。

#### **C. GKD 目标函数**
 GKD 之所以被称为“广义”知识蒸馏，是因为它不仅提出了一种新方法（在线蒸馏），还提供了一个统一的数学框架，将现有的 KD 方法视为该框架的**特例**。

 GKD 的通用目标函数包含两个可调节的部分：

$ L_{GKD}(\theta) = (1-\lambda) \cdot L_{\text{Supervised}} + \lambda \cdot L_{\text{On-Policy}} $

其中 $ \lambda $ 控制使用学生自生成数据的比例（即 On-policy 的程度）。

它在以下两个维度上进行了“泛化”：

1. **数据的泛化 (Data Fraction)：**
    - GKD 允许混合使用“固定数据”和“学生自生成数据”。
    - 当 $ \lambda = 0 $ 时，GKD 就退化变成了 **Supervised KD**（只用固定数据）<sup></sup>。
    - 当 $ \lambda = 1 $ 时，GKD 就变成了纯粹的 **On-policy KD**（只用学生数据）<sup></sup>。
    - GKD 涵盖了这中间的所有可能性（例如 $ \lambda = 0.5 $ 的混合策略）。
2. **损失函数的泛化 (Divergence)：**
    - 传统的 KD 通常局限于使用 **前向 KL 散度**。
    - GKD 框架允许灵活选择差异度量，包括 **反向 KL ** 和 **广义 JSD**。

### SKD, SeqKD 与 On-policy GKD 完整对比
| **特性维度**       | **SKD**                                            | **SeqKD**                                        | **On-policy GKD (λ=1)**                              |
| ------------------ | -------------------------------------------------- | ------------------------------------------------ | ---------------------------------------------------- |
| **训练数据来源**   | 固定的Ground-truth数据集                           | 教师模型生成的固定输出序列                       | **学生模型自生成的输出序列**                         |
| **训练时的上下文** | 始终是Ground-truth（完美上下文）                   | 始终是教师生成的序列                             | **学生模型采样产生的历史序列（可能包含错误）**       |
| **反馈/监督信号**  | 教师在真值序列上的**Token 级概率分布**             | 教师生成的**高概率硬标签序列**                   | 教师在**学生生成的序列**上的 **Token 级概率分布**    |
| **策略类型**       | Off-policy                                         | Off-policy                                       | **On-policy**                                        |
| **分布匹配度**     | **不匹配**：训练看真值，推理靠自己，易产生级联错误 | **不匹配**：训练看教师，推理靠自己，存在分布偏移 | **高度匹配**：训练和推理都在学生自己的输出分布上进行 |
| **损失函数**   | 通常固定为前向 KL 散度                             | 针对教师序列的负对数似然                         | **灵活可选**：前向 KL、反向 KL 或 JSD                |
| **计算开销**       | 较低（仅需教师一次推理）                           | 较高（需预先生成大量教师序列）                   | 中等（训练中需实时进行学生采样与教师推理）           |
| **核心优势**       | 传递教师丰富的分布细节（暗知识）                   | 减少真值数据中的噪声，学习教师的生成风格         | **消除训练-推理不匹配，有效减少幻觉并提升鲁棒性**    |

---

### 符号说明
+ $ \theta $：学生模型中待学习的参数。
+ $ x $：输入的提示词序列。
+ $ y $：输出的响应序列。
+ $ y_{<n} $：输出序列中第 $ n $ 个 Token 之前的历史上下文。
+ $ L_y $：序列 $ y $ 的长度。
+ $ p_S $：学生模型的概率策略。
+ $ p_T $：教师模型概率策略。
+ $ \mathcal{D}_{KL} $：KL 散度（Kullback-Leibler Divergence），SKD 默认使用的度量 。
+ $ \mathcal{D} $：广义差异度量函数，在 GKD 中可以是前向 KL、反向 KL 或广义 JSD。

